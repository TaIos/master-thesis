\newpage


\section{Hyper-parameters}\label{sec:hyper-parameters}

Proposed genetic algorithm~\ref{alg:genetic} has eight hyperparameters.
They are described in table~\ref{tab:hyperparameters-description} and used in listing~\ref{lst:computation-submission-dataset}.
The rest of the sections discuss these hyperparameters further and tries to find their reasonable values.

Two instances are chosen for hyperparameter testing â€“ random\_10 and random\_20.
The reason is that they are not biased towards any preferred solution, e.g., forming clusters.
Thus, insights into the proposed solution can be gained.
However, fine-tuning the hyperparameters to the specific instance or scenario is recommended but only sometimes computationally feasible.

Hyperparameter testing is performed by changing only the hyperparameter under test.
Hyperparameters not under test are identical to the values in listing~\ref{lst:computation-submission-dataset}.
The exceptions are penalization constants $\lambda, \gamma$ (eq.~\ref{eq:objective}), and \verb|populationSize|.
Penalization constants are set to the length of the layout diagonal (see~\ref{subsec:overlapping-penalization-constant}).
Population size is set to $50N$, where $N$ is the size of the instance.
The reason for choosing such parameters as base parameters for testing
is preliminary results (not presented in this thesis), which proved correct in many cases.

Lastly, to achieve the statistical significance of the results presented in this section, each computation\footnotemark[1] is submitted five times with a different random seed.
Presented values are thus an average from five samples.

\footnotetext[1]{
    Copmutations were run on a notebook with Fedora 36, 16GB RAM, AMD Ryzen 7 PRO 4750U 8 x 1.7 - 4.1 GHz, Renoir PRO (Zen 2).
}


\begin{table}[h!]
    \caption[Hyperparameters of the genetic algorithm]{Hyperparameters of the genetic algorithm~\ref{alg:genetic}}
    \label{tab:hyperparameters-description}
    \begin{tabular}{ll}
        \hline
        \textbf{Hyperparameter}   & \textbf{Description}                                       \\ \hline
        \verb|maxNumberOfIter|    & maximum number of iterations                               \\ \hline
        \verb|populationSize|     & population size                                            \\ \hline
        \verb|maximumWildCardCount| & \begin{tabular}[c]{@{}l@{}}
                                          limit on the maximum number of $*$ cut types\\ produced by $OR_{prob}$ decoding
        \end{tabular} \\ \hline
        \verb|orientationWeights| & penalization vector $P$ from eq.~\ref{eq:crossover-orprob} \\ \hline
        \verb|populationDivisionCounts| & \begin{tabular}[c]{@{}l@{}}
                                              reproductive plan ratios\\ (right part of fig.~\ref{fig:population-schema})
        \end{tabular} \\ \hline
        \verb|initialPopulationDivisionCounts| &
        \begin{tabular}[c]{@{}l@{}}
            initial population ratios\\ (left part of fig.~\ref{fig:population-schema})
        \end{tabular} \\ \hline
        \verb|overlappingPenalizationConstant| &
        \begin{tabular}[c]{@{}l@{}}
            overlapping paintings penalization\\ constant $\lambda$ from eq.~\ref{eq:objective}
        \end{tabular} \\ \hline
        \verb|outsideOfAllocatedAreaPenalizationConstant| &
        \begin{tabular}[c]{@{}l@{}}
            outside of allocated area penalization\\ constant $\gamma$ from eq.~\ref{eq:objective}
        \end{tabular} \\ \hline
    \end{tabular}
\end{table}

\newpage
\subsection{Max number of iter}\label{subsec:max-number-of-iter}
Hyperparameter \verb|maxNumberOfIter| determines the number of iterations in the genetic algorithm~\ref{alg:genetic}.

Results for two random instances are in figure~\ref{fig:hyperparameters-max-number-of-iter}.
We can see the initial decrease of the average population objective for both random instances.
Above iteration 300, the initial decreasing trend stops for the random\_10 instance, and for the random\_20 instance, the decrease becomes very slow.

The conclusion is that at least 300 iterations are needed before the average population objective
stops decreasing rapidly.

\subsection{Population size}\label{subsec:population-size}

Hyperparameter \verb|populationSize| is calculated as $\kappa N$, where $\kappa$ is \definice{population scaling factor}
and $N$ is instance size.
It determines the population size that is linear to the instance size.

Results for two random instances are in figure~\ref{fig:hyperparameters-population-size}.
We can see that scaling factor $10$ does not allow
the population objective average to decrease to the levels comparable to scaling factors $50$ and $100$.
It might imply that the scaling factor $10$ cannot represent knowledge gathered over time
in the genetic algorithm or that more iterations are needed.

The conclusion is that using scaling factor between $50$ and $100$ is sufficient, with bias towards $100$
for obtaining better average objective performance.
However, increasing the scaling factor leads to slower computation speed as every population contains
more individuals for which reproductive plan must be computed.

\subsection{Maximum wildcard count}\label{subsec:maximum-wild-card-count}
Hyperparameter \verb|maximumWildCardCount| limits the maximum number of $*$ cut types produced by $OR_{prob}$ decoding (subsec.~\ref{subsec:individual-decoding}).
Keeping this hyperparameter low or even setting it to zero is recommended.
The reason is that if it is high, computation time increases as $*$ spreads in the population.
For example, consider a decoded individual whose orientations $OR$ are solely composed of~$*$ cut types.
Then, the  individual decodes to $2^{|OR|}$ resolved slicing trees as seen in figure~\ref{fig:layout-construction-steps}.

Results for random\_10 instance are in figure~\ref{fig:hyperparameters-maximum-wild-card-count}.
The top sub-figure shows that the average population objective does not differ significantly for any limit on the wildcard cut type.
However, there is a slight advantage for the maximum wildcard count equal to one.
It might be caused by using wildcard penalization $0.5$ (listing~\ref{lst:computation-submission-dataset})
that does not allow the spread of the wildcard in the population.

On the bottom sub-figure, we can see the computation speed as the limit on the wildcard cut type increases.
It grows linearly up to the maximum wildcard count of eight, and then the increase stops.
It might be because the wildcard penalization $0.5$ does not
allow the wildcard cut type to spread over the maximum wildcard count of eight.
Another reason might be computational anomalies caused by high-memory consumption as the maximum wildcard count increases.

The conclusion tested on random\_10 instance is that (a) the maximum wildcard count for wildcard penalization $0.5$
performs similarly for all values, with a slight performance gain if using a maximum wildcard count equal to one,
and (b) computation time is linear with increasing maximum wildcard count and wildcard penalization $0.5$.

\newpage
\subsection{Orientation weights}\label{subsec:orientation-weights}
Hyperparameter \verb|orientationWeights| is the orientation penalization vector $P$ from crossover eq.~\ref{eq:crossover-orprob}.
It determines the bias towards the type of cut ($H$, $V$, $*$, see~\ref{subsec:individual-decoding}).

Only penalization for the wildcard cut type $*$ is tested, as there is no need to penalize or have a preference for $H$ or $V$ cut types.
Also, recall that the hyperparameter \verb|maximumWildCardCount| is set to one during testing, as described at the beginning of the section and showed in listing~\ref{lst:computation-submission-dataset}.

Performance results for two random instances are in figure~\ref{fig:hyperparameters-orientation-weights}.
We can see that for random\_10 instance,
weight does not significantly influence the average population objective, and after iteration 250, differences become negligible.
However, for random\_20 instance,
weight one (no penalization) has a faster-decreasing trend and produces a population with a better average population objective.

The reason for better average performance at larger instance with weight one (no penalization) might
be that search space increases exponentially (there exists at least $2^{N-1}$ different unresolved slicing trees, for instance of size $N$),
and the introduction of wildcard cut type $*$ starts to manifest itself at larger instances.

Results for the average number of wildcard cut types $*$ at the best individual before decoding at each iteration are in
figure~\ref{fig:hyperparameters-orientation-weights-wildcard-cut-type-spread}.
We can see that for the random\_10 instance, weights below one have less than one wildcard.
However, there are between three and four wildcards for a weight equal to one (no penalization).
Similar can be seen for random\_20 instance.

The reason why there are wildcards present in the figure~\ref{fig:hyperparameters-orientation-weights-wildcard-cut-type-spread}
even for weight equal to zero (maximal penalization) is that wildcard can be introduced to a chromosome by mutation or injection of random individuals (see reproductive plan~\ref{subsec:reproductive-plan}).
Described penalization only applies to the crossover.

The conclusion from figure~\ref{fig:hyperparameters-orientation-weights} is that (a) smaller instances, such as random\_10, do not benefit from the introduction of wildcard cut type $*$, and (b) bigger instances, such as random\_20, benefit from no wildcard penalization by having a faster-decreasing trend and producing a better average population.

The conclusion from figure~\ref{fig:hyperparameters-orientation-weights-wildcard-cut-type-spread} is that if we want to be certain that wildcard cut type $*$ is contained in the best individual at each iteration,
wildcard orientation weight must be set close to one.

\subsection{Population division counts}\label{subsec:population-division-counts}
Hyperparameter \verb|populationDivisionCounts| configures ratios in the reproductive plan (subsec.~\ref{subsec:reproductive-plan}).
It influences how the next generation is created in the genetic algorithm~\ref{alg:genetic}
by setting (a) how many elite individuals are copied, (b) how many children are created using a crossover operator,
(c) how many mutants are created using a mutation operator, (d) how many tournament winners are included, and (e) how many random individuals are injected.

Performance results for two random instances are in figure~\ref{fig:hyperparameters-population-division-counts}.
We can see that results do not differ for the smaller or larger instance.
The best reproductive plan strategy is achieved without changing population division hyperparameters from listing~\ref{lst:computation-submission-dataset}.
That means elitism and randomly injected individuals are present.
Additionally, the elitism strategy is the root cause of good performance,
as removing randomly generated individuals does not significantly improve performance.

The reason why the use of elitism is important to obtain good results might be the implementation of the crossover operator (subsec.~\ref{subsec:crossover}).
Crossover adds weights $w_A$ and $w_B$ to each parent based on their objective value (lower objective value achieves bigger weight).
It means that if $w_A \gg w_B$, the offspring is a sample from the search space close to the parent $A$ or nearly identical to $A$.
On the other hand, if the weights are similar and each parent represents a different (sub)-optimal solution,
the transfer of information does not happen, and the offspring is no better than a randomly generated individual.
Elitism avoids crossover by directly copying the best individuals without any modification.
It leads to keeping track of multiple (sub)-optimal solutions simultaneously.
In addition, each time a crossover is applied, one parent is selected from the elite pool,
which supports intensification.

On the other hand, removing elitism produces the worst results.
The reason might be the inability to keep track of competing (sub)-optimal and the fast spread of the first
\definice{macho individual}, an individual that performs significantly better than everyone else.
Without elitism, the most significant way to keep track of found (sub)-optimal solutions is through crossover.
Crossover chooses parents randomly from the average pool, which does not give any constraint on the weights $w_A$ and $w_B$.
Additionally, as mentioned in the previous paragraph, similar weights for two different parents produce no better offspring than randomly generated individuals.
It means that most offspring do not perform well.
On the other hand, as soon as the macho individual appears as the parent in the crossover, the offspring is effectively a copy of a macho individual.
Then, if the macho individual is randomly chosen more than once as a parent in a crossover, it rapidly spreads and takes over the whole population.
The chance of macho-individual taking over the population with elitism is decreased as it deliberately keeps track of multiple competing performant individuals,
making it harder for the macho individual to spread.

The conclusion is that using elitism is essential for obtaining good painting placement solutions
as it can keep track of multiple (sub)-optimal solutions simultaneously.

\subsection{Initial population division counts}\label{subsec:initial-population-division-counts}
Hyperparameter \verb|initialPopulationDivisionCounts| configures generation ratios of the initial population (left part of fig.~\ref{fig:population-schema}).
It consists of randomly generated and greedily generated individuals.

Performance results for two random instances are in figure~\ref{fig:hyperparameters-initial-population-division-counts}.
The different ratios' results do not greatly differ.

The conclusion is that hyperparameter \verb|initialPopulationDivisionCounts| does not significantly affect the obtained results.

\subsection{Overlapping penalization constant}\label{subsec:overlapping-penalization-constant}

Hyperparameter \verb|overlappingPenalizationConstant| is the penalization constant $\lambda$ from eq.~\ref{eq:objective}
used to penalize individuals representing solutions with overlapping paintings.
It is calculated as $\rho D$, where $\rho$ is a diagonal multiple and $D$ is the length of a diagonal in a layout.
Diagonal length $D$ for a layout with width $W$ and height $H$ is $\sqrt{W^2 + H^2}$.

Results of the average overlapping paintings count for the best individual at each iteration are in figure~\ref{fig:hyperparameters-overlapping-penalization-constant}.
We can see that the two lowest diagonal multiples with values $0.5$ and $1.0$ fail to remove most overlapping paintings from the best individuals.
On the other hand, values $2$ and higher can remove overlapping paintings at the smaller instance.
However, in the larger instance, there are still several overlappings present.

The conclusion is that hyperparameter \verb|overlappingPenalizationConstant| should be set at least to
two times the diagonal length of the layout to start penalizing individuals that represent solutions with overlapping paintings.

\subsection{Outside of allocated area penalization constant}\label{subsec:outside-of-allocated-area-penalization-constant}
Hyperparameter~\verb|outsideOfAllocatedAreaPenalizationConstant| is the penalization constant $\gamma$ from eq.~\ref{eq:objective} used to penalize
individuals representing solutions with paintings that are placed outside of their allocated area (see fig.~\ref{fig:allocated-area} and subsec.~\ref{subsec:placement-heuristic}).
It is calculated identically as the overlapping penalization constant (subsec.~\ref{subsec:overlapping-penalization-constant}).
That is, as $\rho D$, where $\rho$ is a diagonal multiple, and $D$ is the length of a diagonal in a layout.

Results of the percentage of paintings placed outside the allocated area for two random instances are in table~\ref{tab:hyperparameters-outside-of-allocated-area-penalization-constant}
(it is presented using a table because the values do not significantly change throughout the iterations).
We can see that the outside of allocated area penalization constant cannot force the creation of the allocated space that would fit the vast majority of the paintings.
Interestingly, there are a few percentage points drops in favor of the larger instance.
The reason might be that it has more degrees of freedom, i.e.,  more possibilities to create a cut.
It can thus create more fitting slicing layouts.

However, the failure of the outside of allocated area penalization constant to force
the creation of sufficient allocated space in most cases is not that important.
The reason is that the placing heuristic (subsec.~\ref{subsec:placement-heuristic})
tries to place the painting at several placement points in the allocated area (see fig.~\ref{fig:allocated-area}).
It can thus balance the few paintings that have been allocated sufficient area to avoid
and account for other parts of the objective function, e.g., overlapping paintings.

The conclusion is that the hyperparameter \verb|outsideOfAllocatedAreaPenalizationConstant| is the least
important and might be left out by setting it to zero to save computation time.


\begin{table}[h!]
    \caption{Percentage of paintings placed outside allocated area}
    \label{tab:hyperparameters-outside-of-allocated-area-penalization-constant}
    \shorthandoff{-}
    \begin{threeparttable}
        \begin{tabular}{lll}
            \cline{1-3}
            & \multicolumn{2}{c}{\textbf{Instance}} \\ \cline{2-3}
            \textbf{\begin{tabular}[c]{@{}l@{}}
                        Diagonal\\ multiple
            \end{tabular}} & \multicolumn{1}{c}{random\_10} & \multicolumn{1}{c}{random\_20} \\ \hline
            0              & 99.6                           & 93.3                           \\ \hline
            0.5            & 99.6                           & 94.1                           \\ \hline
            1              & 98                             & 92.5                           \\ \hline
            2              & 97.4                           & 95.9                           \\ \hline
            3              & 98.9                           & 91.7                           \\ \hline
            4              & 99.8                           & 94.9                           \\ \hline
            10             & 99.7                           & 96.1                           \\ \hline
            50             & 98.6                           & 97.8                           \\ \hline
        \end{tabular}
        \begin{tablenotes}
            \small
            \item Percentage is averaged over all iterations for the best individual at each iteration.
        \end{tablenotes}
    \end{threeparttable}
    \shorthandon{-}
\end{table}

\begin{figure}[h!]
    \centering
    \subfloat{\includegraphics[width=0.8\textwidth]{hyperparameters/max_number_of_iter_random_10}\label{subfig:hyperparameters-max-number-of-iter-random-10}}

    \subfloat{\includegraphics[width=0.8\textwidth]{hyperparameters/max_number_of_iter_random_20}\label{subfig:hyperparameters-max-number-of-iter-random-20}}
    \caption[Testing maximum number of iterations]
    {Testing maximum number of iterations at two random instances.}
    \label{fig:hyperparameters-max-number-of-iter}%
\end{figure}

\begin{figure}[h!]
    \centering
    \subfloat{\includegraphics[width=0.9\textwidth]{hyperparameters/population_size_random_10}\label{subfig:hyperparameters-population-size-random-10}}

    \subfloat{\includegraphics[width=0.9\textwidth]{hyperparameters/population_size_random_20}\label{subfig:hyperparameters-population-size-random-20}}
    \caption[Testing population scaling factor]
    {Testing population scaling factor at two random instances.
    The population size is $\kappa N$ for population scaling factor $\kappa$ and instance of size $N$. }
    \label{fig:hyperparameters-population-size}%
\end{figure}

\begin{figure}[h!]
    \centering
    \subfloat{\includegraphics[width=1.1\textwidth]{hyperparameters/maximum_wild_card_count_performance_random_10}\label{subfig:hyperparameters-maximum-wild-card-count-performance}}

    \subfloat{\includegraphics[width=0.8\textwidth]{hyperparameters/maximum_wild_card_count_computation_time_random_10}\label{subfig:hyperparameters-maximum-wild-card-count-computation-speed}}
    \caption[Testing maximum wildcard count]
    {Testing increasing maximum wildcard count. Performance (top) and computation speed (bottom) are showed.}
    \label{fig:hyperparameters-maximum-wild-card-count}%
\end{figure}


\begin{figure}[h!]
    \centering
    \subfloat{\includegraphics[width=1\textwidth]{hyperparameters/orientation_weights_random_10}\label{subfig:hyperparameters-orientation-weights-random-10}}

    \subfloat{\includegraphics[width=1\textwidth]{hyperparameters/orientation_weights_random_20}\label{subfig:hyperparameters-orientation-weights-random-20}}
    \cprotect\caption[Testing orientation weights performance]
    {Testing performance of orientation weight for a wildcard cut type $*$ at two random instances.
    Hyperparameter \verb|maximumWildCardCount| is 1.}
    \label{fig:hyperparameters-orientation-weights}%
\end{figure}

\begin{figure}[h!]
    \centering
    \subfloat{\includegraphics[width=0.9\textwidth]{hyperparameters/orientation_weights_wildcard_cut_type_spread_random_10}\label{subfig:hyperparameters-orientation-weights-wildcard-cut-type-spread-random-10}}

    \subfloat{\includegraphics[width=0.9\textwidth]{hyperparameters/orientation_weights_wildcard_cut_type_spread_random_20}\label{subfig:hyperparameters-orientation-weights-wildcard-cut-type-spread-random-20}}
    \cprotect\caption[Testing wildcard spread]
    {Testing wildcard cut type $*$ spread at two random instances.
    Each iteration shows an average count of wildcard cut type $*$ at best individual before decoding for different values of wildcard orientation weights.
    Hyperparameter \verb|maximumWildCardCount| is 1.}
    \label{fig:hyperparameters-orientation-weights-wildcard-cut-type-spread}%
\end{figure}

\begin{figure}[h!]
    \centering
    \subfloat{\includegraphics[width=1.05\textwidth]{hyperparameters/population_division_counts_random_10}\label{subfig:hyperparameters-population-division-counts-random-10}}

    \subfloat{\includegraphics[width=1.05\textwidth]{hyperparameters/population_division_counts_random_20}\label{subfig:hyperparameters-population-division-counts-random-20}}
    \caption[Testing population division counts]
    {Testing population division counts at two random instances. Four variants are displayed.
    The first does not use elitism.
    The second does not inject random individuals.
    The third combines the first and second, and the last does not change the population division counts as described in listing~\ref{lst:computation-submission-dataset}.}
    \label{fig:hyperparameters-population-division-counts}%
\end{figure}

\begin{figure}[h!]
    \centering
    \subfloat{\includegraphics[width=0.95\textwidth]{hyperparameters/initial_population_division_counts_random_10}\label{subfig:hyperparameters-initial-population-division-counts-random-10}}

    \subfloat{\includegraphics[width=0.95\textwidth]{hyperparameters/initial_population_division_counts_random_20}\label{subfig:hyperparameters-initial-population-division-counts-random-20}}
    \caption[Testing initial population division counts]
    {Testing initial population division counts at two random instances.
    The initial population consists of randomly and greedily generated individuals (left part of fig.~\ref{fig:population-schema}).}
    \label{fig:hyperparameters-initial-population-division-counts}%
\end{figure}

\begin{figure}[h!]
    \centering
    \subfloat{\includegraphics[width=0.95\textwidth]{hyperparameters/overlapping_penalization_constant_random_10}\label{subfig:hyperparameters-overlapping-penalization-constant-random-10}}

    \subfloat{\includegraphics[width=0.95\textwidth]{hyperparameters/overlapping_penalization_constant_random_20}\label{subfig:hyperparameters-overlapping-penalization-constant-random-20}}
    \caption[Testing overlapping penalization constant]
    {Testing overlapping penalization constant $\lambda$ (eq.~\ref{eq:objective}) at two random instances.
    It is calculated as $\rho D$, where $\rho$ is a diagonal multiple, and $D$ is the length of a diagonal in a layout.
    Graphs show the average overlapping paintings count for the best individual at each iteration.}
    \label{fig:hyperparameters-overlapping-penalization-constant}%
\end{figure}


